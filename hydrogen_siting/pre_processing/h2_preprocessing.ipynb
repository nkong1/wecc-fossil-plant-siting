{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b587575b",
   "metadata": {},
   "source": [
    "This script further processes the initial 1x1km suitability layers of each hydrogen production technology. The steps are as follows:\n",
    "1) The 1x1km features are broken down into sizes corresponding to the reference plant footprint of each production technology.\n",
    "\n",
    "2) A high geospatial resolution filter (30m) is applied using the NLCD and Farms Under Threat datasets.\n",
    "\n",
    "3) Candidates that do not fall squarely within a load zone are removed.\n",
    "\n",
    "4) ** Requires manual intervention ** Export the intermediate outputs from Step 3 to pgAdmin and use Postgres to add distances to feedstock sources and substations using the script in the 'postgres' folder. Put outputs in the folder 'candidate_sites_with_dists'\n",
    "\n",
    "5) Remove candidate sites that overlap with substations or above-ground feedstock sources (an overlap with a natural gas pipelines is allowed because these are typically located underground, but an overlap with a biogas plant is not). These are the final candidate sites.\n",
    "\n",
    "6) Determine the potential for each technology group. Overlaps between technologies are resolved by grouping the candidate layers and iteratively filtering them in order of increasing candidate count, so that sites from sparser layers are preserved at a higher proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441d725",
   "metadata": {},
   "source": [
    "Set-Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca92c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and methods\n",
    "\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.windows import from_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ab36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data paths\n",
    "base_path = Path.cwd() # this is the pre-processing folder path\n",
    "\n",
    "# Input data paths\n",
    "candidate_sites_path = base_path / 'input_files' / \"candidate_sites_1x1km\"\n",
    "combined_exclusion_30m_path = base_path / 'input_files' / 'combined_exclusion_30m.tif'\n",
    "load_zones_path = base_path / \"input_files\" / \"load_zones\" / \"load_zones.shp\"\n",
    "\n",
    "# Intermediate output paths\n",
    "ref_footprints_output_path = base_path / 'intermediate_outputs' / 'ref_footprints' \n",
    "filtered_nlcd_ag_path = base_path / 'intermediate_outputs' / 'nlcd_ag_filtered'\n",
    "with_dists_path = base_path / 'intermediate_outputs' / 'with_dists'\n",
    "grouped_output_path = base_path / 'candidate_sites_by_group'\n",
    "\n",
    "# Final output path for suitable candidate sites\n",
    "final_candidates_by_tech_path = base_path.parent / 'final_candidates'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for path in [ref_footprints_output_path, filtered_nlcd_ag_path, with_dists_path, grouped_output_path, final_candidates_by_tech_path]:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reference plant specifications\n",
    "\n",
    "# Create a dictionary mapping each hydrogen production technology to its square reference plant footprint length (m)\n",
    "ref_footprint = {\n",
    "    \"gas_smr\": 354.5,\n",
    "    \"gas_smr_ccs\": 354.5,\n",
    "    \"bio_smr\": 354.5,\n",
    "    \"bio_smr_ccs\": 354.5,\n",
    "    \"gas_atr_ccs\": 354.5,\n",
    "    \"bio_atr_ccs\": 354.5,\n",
    "    \"coal_gas\": 614.0,\n",
    "    \"coal_gas_ccs\": 614.0,\n",
    "    \"biomass_gas\": 297.6,\n",
    "}\n",
    "\n",
    "# Create a dictionary mapping each hydrogen production technology to its reference plant capacity (tonnes/day)\n",
    "ref_capacity = {\n",
    "    \"gas_smr\": 150,\n",
    "    \"gas_smr_ccs\": 150,\n",
    "    \"bio_smr\": 150,\n",
    "    \"bio_smr_ccs\": 150,\n",
    "    \"gas_atr_ccs\": 205,\n",
    "    \"bio_atr_ccs\": 205,\n",
    "    \"coal_gas\": 205,\n",
    "    \"coal_gas_ccs\": 205,\n",
    "    \"biomass_gas\": 48,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7cc46",
   "metadata": {},
   "source": [
    "Step 1: Break down each 1x1km suitable square into smaller squares based on the reference plant footprint lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f593f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function that a length (in meters) to the nearest length greater than or equal to the original length that evenly divide a \n",
    "# 1x1 km grid into squares.\n",
    "\n",
    "def nearest(length):\n",
    "    if length <= 250:\n",
    "        return 250\n",
    "    elif length <= 1000/3:\n",
    "        return 1000/3\n",
    "    elif length <= 500:\n",
    "        return 500\n",
    "    elif length <= 1000:\n",
    "        return 1000\n",
    "\n",
    "# Use the helper to map the reference footprint length for each tech to the length we'll use to break down the 1x1km squares.\n",
    "rounded_ref_capacity = {k: nearest(v) for k, v in ref_footprint.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing\n",
    "for file_path in candidate_sites_path.glob(\"*.gpkg\"):\n",
    "    print('Processing:', file_path.name)\n",
    "    # Extract the technology name from the file name\n",
    "    tech_name = file_path.stem\n",
    "    \n",
    "    # Load the candidate sites GeoPackage\n",
    "    gdf = gpd.read_file(file_path)\n",
    "    \n",
    "    # Get the reference plant footprint length for the current technology\n",
    "    ref_length = rounded_ref_capacity.get(tech_name)\n",
    "    \n",
    "    # Calculate the number of smaller squares along each side of the 1x1 km square\n",
    "    num_squares_per_side = int(1000 / ref_length)\n",
    "    \n",
    "    suitable_sites = []\n",
    "    \n",
    "    # Iterate over each geometry in the GeoDataFrame\n",
    "    for _, row in gdf.iterrows():\n",
    "        \n",
    "        minx, miny, maxx, maxy = row.geometry.bounds\n",
    "        \n",
    "        # Generate smaller squares within the 1x1 km square\n",
    "        for i in range(num_squares_per_side):\n",
    "            for j in range(num_squares_per_side):\n",
    "                new_minx = minx + i * ref_length\n",
    "                new_miny = miny + j * ref_length\n",
    "                new_maxx = new_minx + ref_length\n",
    "                new_maxy = new_miny + ref_length\n",
    "                \n",
    "                new_square = box(new_minx, new_miny, new_maxx, new_maxy)\n",
    "                suitable_sites.append(new_square)\n",
    "    \n",
    "    # Create a GeoDataFrame from the suitable sites\n",
    "    suitable_gdf = gpd.GeoDataFrame(geometry=suitable_sites, crs=gdf.crs)\n",
    "                      \n",
    "    suitable_gdf.to_file(ref_footprints_output_path / f\"{tech_name}.gpkg\", driver=\"GPKG\")\n",
    "    \n",
    "    print(f\"Processed sites with reference footprints for {tech_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5e9a",
   "metadata": {},
   "source": [
    "Step 2: Apply the NLCD and Farms Under Threat datasets (each has a 30m resolution) to filter the candidates in the newly obtained layers. \n",
    "\n",
    "Using the NLCD, we filter out: Open Water, Pernnial Ice/Snow, Developed Open Space, Developed Low Intensity, Developed Medium Intensity, Developed High Intensity, Deciduous Forest, Evergreen Forest, Mixed Forest, Woody Wetlands, and Herbaceous Wetlands.\n",
    "\n",
    "Using the Farms under Threat dataset, we filter out \"Nationally Significant Agricultural Land.\"\n",
    "\n",
    "For ease of processing, we combined the two datasets into one raster file. Excluded pixels have a value of 1.\n",
    "\n",
    "Note: If 95% or more of a candidate site passes the filter, it is deemed acceptable and kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5674a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to calculate the acceptance of a candidate site using an exclusion raster\n",
    "def calculate_acceptance(geometry, exclusion_src, threshold=0.95):\n",
    "    # Get bounding box\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "    \n",
    "    # Compute window for each raster\n",
    "    exclusion_window = from_bounds(minx, miny, maxx, maxy, exclusion_src.transform)\n",
    "    \n",
    "    # Read only windowed data\n",
    "    exclusion_data = exclusion_src.read(1, window=exclusion_window)\n",
    "    \n",
    "    window_transform = exclusion_src.window_transform(exclusion_window)\n",
    "    mask = geometry_mask(\n",
    "        [geometry],\n",
    "        transform=window_transform,\n",
    "        invert=True,\n",
    "        out_shape=exclusion_data.shape,\n",
    "        all_touched=True\n",
    "    )\n",
    "\n",
    "    exclusion_window_values = exclusion_data[mask]\n",
    "    \n",
    "    valid_ratio = np.count_nonzero(exclusion_window_values == 0) / exclusion_window_values.size \n",
    "    return valid_ratio >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the exclusion raster\n",
    "exclusion_src = rasterio.open(combined_exclusion_30m_path)\n",
    "\n",
    "# Process each layer, using the intermediate files created in the previous step\n",
    "for file_path in ref_footprints_output_path.glob(\"*.gpkg\"):\n",
    "    print('Processing:', file_path.name)\n",
    "    tech_name = file_path.stem\n",
    "    \n",
    "    refined_gdf = gpd.read_file(file_path)\n",
    "    \n",
    "    accepted_geometries = []\n",
    "    for _, row in refined_gdf.iterrows():\n",
    "        if calculate_acceptance(row.geometry, exclusion_src):\n",
    "            accepted_geometries.append(row.geometry)\n",
    "    \n",
    "    final_gdf = gpd.GeoDataFrame(geometry=accepted_geometries, crs=refined_gdf.crs)\n",
    "    final_gdf.to_file(filtered_nlcd_ag_path / f\"{tech_name}.gpkg\", driver=\"GPKG\")\n",
    "    print(f\"Saved filtered suitable sites for {tech_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a580070",
   "metadata": {},
   "source": [
    "Step 3: Add a load area column to each file, filtering out candidates that do not fall squarely within a load zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the load zones file\n",
    "load_zones_gdf = gpd.read_file(load_zones_path)\n",
    "\n",
    "# Process each layer\n",
    "for file_path in filtered_nlcd_ag_path.glob(\"*.gpkg\"):\n",
    "    print('Processing:', file_path.name)\n",
    "    tech_name = file_path.stem\n",
    "    \n",
    "    final_gdf = gpd.read_file(file_path)\n",
    "    \n",
    "    # Perform spatial join to associate each candidate site with a load zone\n",
    "    joined_gdf = gpd.sjoin(final_gdf, load_zones_gdf[['geometry', 'LOAD_AREA']], how=\"left\", predicate=\"within\").reset_index(drop=True)\n",
    "    \n",
    "    # Filter out geometries that do not fall within any load zone\n",
    "    joined_gdf = joined_gdf[~joined_gdf[\"LOAD_AREA\"].isna()].copy().drop(columns=[\"index_right\"])\n",
    "\n",
    "    # Save the updated gdf back to the same file\n",
    "    joined_gdf.to_file(file_path, driver=\"GPKG\")\n",
    "    print(f\"Added load area info and saved for {tech_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761eaec",
   "metadata": {},
   "source": [
    "Step 4: Use PostgreSQL to add distances to feedstock sources and substations. Use the scripts in the 'postgres' folder. Put the resulting files in the folder 'candidate_sites_with_dists'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ad14b",
   "metadata": {},
   "source": [
    "Step 5: Remove overlaps with existing physical features (substations, coal mines, biomass plants, etc). Save these to the final outputs for candidate sites by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tech_file in with_dists_path.glob(\"*.gpkg\"):\n",
    "    tech_name = tech_file.stem\n",
    "    gdf = gpd.read_file(tech_file)\n",
    "\n",
    "    # Remove any candidate sites that overlap with substations\n",
    "    gdf = gdf[gdf[\"dist_to_substation_meters\"] > 0].copy()\n",
    "\n",
    "    # Remove any candidates that overlap with feedstock sources (except for natural gas, which may have pipelines underground)\n",
    "    if tech_name not in ['gas_smr', 'gas_smr_ccs', 'gas_atr_ccs']:\n",
    "        gdf = gdf[gdf[\"dist_to_feedstock_meters\"] > 0].copy()\n",
    "\n",
    "    gdf.to_file(final_candidates_by_tech_path / f\"{tech_name}.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3200c",
   "metadata": {},
   "source": [
    "Step 6: Group the technology layers by feedstock source and remove overlaps between them to estimate the potential for each technology group per load zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af351a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a helper function to remove overlaps between layers\n",
    "def remove_overlaps(base_gdfs, top_gdf):\n",
    "    \"\"\"\n",
    "    Removes overlapping features from the top GeoDataFrame \n",
    "    where they overlap with the base GeoDataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    - base_gdf : A list of base layers (overlaps from top will be removed here).\n",
    "    - top_gdf : The top layer (features overlapping base will be removed).\n",
    "    \n",
    "    Returns\n",
    "    A GeoDataFrame consisting of:\n",
    "    - only the non-overlapping portions of features from top_gdf\n",
    "    \"\"\"\n",
    "    # Start with first base gdf, geometry only\n",
    "    combined_base = base_gdfs[0][[\"geometry\"]]\n",
    "\n",
    "    # Overlay the rest, geometry only\n",
    "    for base_gdf in base_gdfs[1:]:\n",
    "        combined_base = gpd.overlay(combined_base, base_gdf[[\"geometry\"]], how=\"union\")\n",
    "\n",
    "    # Spatial join to find overlapping top features\n",
    "    overlaps = gpd.sjoin(top_gdf, combined_base, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    # Keep only those NOT in overlaps\n",
    "    cleaned_top = top_gdf.loc[~top_gdf.index.isin(overlaps.index)]\n",
    "    \n",
    "    return cleaned_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the gdfs in order of increasing number of candidate sites\n",
    "biomass_gdf = gpd.read_file(final_candidates_by_tech_path / 'biomass_gas.gpkg')\n",
    "bio_smr_gdf = gpd.read_file(final_candidates_by_tech_path / 'bio_smr.gpkg')\n",
    "bio_smr_ccs_gdf = gpd.read_file(final_candidates_by_tech_path / 'bio_smr_ccs.gpkg')\n",
    "coal_gas_ccs_gdf = gpd.read_file(final_candidates_by_tech_path / 'coal_gas_ccs.gpkg')\n",
    "coal_gas_gdf = gpd.read_file(final_candidates_by_tech_path / 'coal_gas.gpkg')\n",
    "gas_smr_gdf = gpd.read_file(final_candidates_by_tech_path / 'gas_smr.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8991d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove overlaps in order of increasing number of candidate sites\n",
    "\n",
    "# The filtered sites for biomass are the same as the original\n",
    "biomass_gdf.to_file(grouped_output_path / 'biomass_gas.gpkg', driver='GPKG')\n",
    "\n",
    "# Get the combined sites for all biomass smr/atr\n",
    "all_bio_smr_gdf = remove_overlaps([biomass_gdf], bio_smr_ccs_gdf)\n",
    "all_bio_smr_gdf.to_file(grouped_output_path / 'bio_smr_atr.gpkg', driver='GPKG')\n",
    "\n",
    "# Get the combined sites for all coal gasification \n",
    "all_coal_gas_gdf = remove_overlaps([biomass_gdf, all_bio_smr_gdf], coal_gas_ccs_gdf)\n",
    "all_coal_gas_gdf.to_file(grouped_output_path / 'coal_gas.gpkg', driver='GPKG')\n",
    "\n",
    "# Get the sites for all gas smr/atr\n",
    "all_gas_smr_gdf = remove_overlaps([biomass_gdf, all_bio_smr_gdf, all_coal_gas_gdf], gas_smr_gdf)\n",
    "all_gas_smr_gdf.to_file(grouped_output_path / 'gas_smr_atr.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b6f6b",
   "metadata": {},
   "source": [
    "Step 7: Calculate the potential of each technology per each load zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32744342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper function that calculates the potential capacity per load zone for given tech(s) in MW\n",
    "def calculate_potential(candidates_gdf, tech_names, ref_capacity):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    - candidates_gdf: the gdf of candidate sites for the given tech(s)\n",
    "    - tech_names: the hydrogen production technologies that the layer is for\n",
    "    - ref_capacity: the reference capacity of the candidates (tonnes/day)\n",
    "        - each technology in tech_names must have the same reference capacity\n",
    "\n",
    "    Outputs:\n",
    "    - df: a df with the potential capacity per tech by load zone, structured with the following columns:\n",
    "        - LOAD_AREA, prod_tech1, prod_tech2, prod_tech3, site_count, potential_MW\n",
    "        - prod_tech2 and prod_tech3 may be empty (contain empty strings)\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of candidate sites in each load zone\n",
    "    count_by_load_area = candidates_gdf.groupby(\"LOAD_AREA\").size().reset_index(name=\"site_count\")\n",
    "\n",
    "    for i in range(1, 4):  \n",
    "        if i <= len(tech_names):\n",
    "            count_by_load_area[f\"prod_tech{i}\"] = tech_names[i-1]\n",
    "        else:\n",
    "            count_by_load_area[f\"prod_tech{i}\"] = \"\"\n",
    "\n",
    "\n",
    "    # Calculate reference capacity in MW (using 33.39 kg H2/MWh and truncating)\n",
    "    tech_ref_capacity_MW = int(ref_capacity / 24 * 33.39)\n",
    "\n",
    "    # Calculate total potential capacity in each load zone\n",
    "    count_by_load_area[\"potential_MW\"] = (\n",
    "        count_by_load_area[\"site_count\"] * tech_ref_capacity_MW\n",
    "    )\n",
    "\n",
    "    return count_by_load_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a running list of potential capacity\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "# Load final suitable candidate sites for each technology and calculate potential capacity by load zone\n",
    "for tech_file in grouped_output_path.glob(\"*.gpkg\"):\n",
    "    gdf = gpd.read_file(tech_file)\n",
    "\n",
    "    file_name = tech_file.stem\n",
    "    \n",
    "    if file_name == 'bio_smr_atr':\n",
    "        tech_names = ['bio_smr', 'bio_smr_ccs', 'bio_atr_ccs']\n",
    "    elif file_name == 'coal_gas':\n",
    "        tech_names = ['coal_gas', 'coal_gas_ccs']\n",
    "    elif file_name == 'gas_smr_atr':\n",
    "        tech_names = ['gas_smr', 'gas_smr_ccs', 'gas_atr_ccs']\n",
    "    elif file_name == 'biomass_gas':\n",
    "        tech_names = ['biomass_gas']\n",
    "    else:\n",
    "        raise Exception(f'tech name {file_name} not found')\n",
    "\n",
    "    nameplate_capacity = ref_capacity[tech_names[0]]\n",
    "\n",
    "    potential_df = calculate_potential(gdf, tech_names, nameplate_capacity)\n",
    "    \n",
    "    # Append to output DataFrame\n",
    "    output_df = pd.concat([output_df, potential_df], ignore_index=True)\n",
    "\n",
    "# Sort \n",
    "output_df = output_df.sort_values(by=[\"LOAD_AREA\", \"prod_tech1\", \"prod_tech2\", \"prod_tech3\"])\n",
    "\n",
    "# Build the cartesian product of load areas × unique technology sets\n",
    "load_areas = load_zones_gdf[\"LOAD_AREA\"].unique()\n",
    "\n",
    "# Get unique tech sets (as tuples) from your output_df\n",
    "tech_sets = (\n",
    "    output_df[[\"prod_tech1\", \"prod_tech2\", \"prod_tech3\"]]\n",
    "    .drop_duplicates()\n",
    "    .apply(tuple, axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Build MultiIndex product of load_areas × tech_sets\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [load_areas, tech_sets],\n",
    "    names=[\"LOAD_AREA\", \"tech_set\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Expand the tuple back into columns\n",
    "all_combinations[[\"prod_tech1\", \"prod_tech2\", \"prod_tech3\"]] = pd.DataFrame(\n",
    "    all_combinations[\"tech_set\"].tolist(), index=all_combinations.index\n",
    ")\n",
    "all_combinations = all_combinations.drop(columns=\"tech_set\")\n",
    "\n",
    "# Merge with output_df on LOAD_AREA + all three prod_tech columns\n",
    "output_df = all_combinations.merge(\n",
    "    output_df,\n",
    "    on=[\"LOAD_AREA\", \"prod_tech1\", \"prod_tech2\", \"prod_tech3\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# Save\n",
    "output_csv_path = base_path.parent / \"h2_tech_potentials.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Saved technology capacity by load zone to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
